# -*- coding: utf-8 -*-
"""Copy of fcc_sms_text_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZaBjTrCvwtdE-dHqOOMOSNIzm1B01TBZ
"""

# import libraries
try:
    # %tensorflow_version only exists in Colab.
    !pip install tf-nightly
except Exception:
    pass
import tensorflow as tf
import pandas as pd
from tensorflow import keras
!pip install tensorflow-datasets
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

# get data files
!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv
!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv

train_file_path = "train-data.tsv"
test_file_path = "valid-data.tsv"

traindata = pd.read_table(train_file_path)
vailddata = pd.read_table(test_file_path)

vailddata.head()

#i think by doing columns this way i lose the first entry but well its not a big deal i think
traindata.columns = ['Classification', 'message']
vailddata.columns = ['Classification', 'message']

traindata.head()

fulldata = traindata.message.append(vailddata.message, ignore_index=True) #i combine both datasets because i want to fill tokenizer (which i explain later) on entire data

a = fulldata.str.split().apply(len) #Later i will need to know how many words emailes have, because i need them to have the same number of words i think good idea is to take mean number of words and trim excess words

print(max(a))
print(a.quantile(0.75)) #actually i decided to go with 3th quantile, where 75% of data is below this number and only 25% is excess

length_of_emails = int(a.quantile(0.75))

print(len(fulldata)) #not important but we have 5569 messages in total

from keras.preprocessing.text import Tokenizer

# Initialize tokenizer, what this does is essentialy it maps each word to some number, so we get a full vocabulary of numbers where each is some word. its important cuz you can,t do anything with text strings
tokenizer = Tokenizer()

# Fit the Tokenizer on the entire data so it has all the words in vocab
tokenizer.fit_on_texts(fulldata)

# Convert the training data to sequences of tokens
train_X = tokenizer.texts_to_sequences(traindata.message)

# Convert the validation data to sequences of tokens
val_X = tokenizer.texts_to_sequences(vailddata.message)

print(val_X[0]) #it looks like this
print(vailddata.message.loc[0])

#now i use this method to trim any email that has more than a mean amount of words in data (16) , padding fills this array of emails as numbers with 0, where 'post' means fill at the end if its less than 16. 
#truncating trims emails with more than 16 words, again post meaning trim at the end.  'pre' would trim at the beginning.
train_X = keras.preprocessing.sequence.pad_sequences(train_X, maxlen=length_of_emails, padding='post', truncating='post') 
val_X =  keras.preprocessing.sequence.pad_sequences(val_X, maxlen=length_of_emails, padding='post', truncating='post')

print(val_X) #weird but ok

vocab_size = len(tokenizer.word_index) + 1 #soo here is how to get vocabulary size out of this tokenizer. as you see it has more entries than fulldata words but this is because it has all the unique words, earlier we counted
#just a total number of words
print(vocab_size)

train_Y = traindata.Classification #remove labels from training and validation data
val_Y = vailddata.Classification

val_Y.replace(to_replace = ['ham','spam'], value = [0,1],inplace = True) #now labels are string, again they need to be numbers so replace ham with 0 and spam with 1
train_Y.replace(to_replace = ['ham','spam'], value = [0,1], inplace = True)
train_Y.head()

from sklearn.model_selection import train_test_split #its good to have a test data so i take a small fraction of train data to get test data by using this method, pretty self explainatory.
train_X, test_X, train_Y, test_Y = train_test_split(train_X, train_Y, random_state=2, test_size = 0.1)

print(len(train_X))
print(len(test_X))
print(len(val_X))

#okay the model, 
model = keras.Sequential([
    
  keras.layers.Embedding(input_dim=vocab_size, output_dim=75, input_length=length_of_emails), #generally it preprocess the data so the integers are mapped into vectors , output dim lets you choose how many numbers this vector has
  keras.layers.LSTM(units=256, dropout=0.3, recurrent_dropout=0.3), #
  keras.layers.Dense(units=1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001),metrics=[ 'acc'])
model.summary()

model.fit(train_X, train_Y, epochs=10, batch_size = 32,validation_data=(val_X, val_Y)) #train the data

y_pred = model.predict(test_X) #predictions on test data to see how good model is later
y_prediction = []
for i in y_pred:
  if i > 0.3:
    y_prediction.append(1)
  else:
    y_prediction.append(0)

print(y_prediction)
print(test_Y)

from sklearn.metrics import precision_recall_fscore_support

# so this function allows me to see precision and recall
#precision is basically number of  true positive (predictions belonging to class closer to 1 in binary classification , which are correct)/all the positives model made eg. 80/100 means 80% precision and that 80 of 100 predictions for eg of spam were correct.
#recall is  number of true positves but this time divided by an actual number of positives so lets say we have 100 spam emails. if model predicts 70 of them then recall is 70/100 = 70%
#f1 score is just some math including previous metrics considered as good indicator 
precision, recall, f1_score, support = precision_recall_fscore_support(test_Y, y_prediction, average='binary')

print(f'Precision: {precision:.3f}')
print(f'Recall: {recall:.3f}')

F1 = 2 * (precision * recall) / (precision + recall)
print(f'F1 score the (higher the better between 0 and 1) is:  {f1_score:.3f}')

#seems good

#IMPORTANT, due to this task making me predict specific emails, i see that when i set the threshold of whether an email is a spam or not to 0.3 i cannot complete the task but it typically has a better F1 score.
#when i set it to 0.0001 the task is completed but the F1 score drops

pred_text = ["how are you doing today?"] #it needs to be in a list or tokenizer encodes each letter
pred_text  = tokenizer.texts_to_sequences(pred_text)
#pred_text = [[elem for sublist in pred_text for elem in sublist]] it flattened some version of a list no longer neccasary
print(pred_text)
pred_text = keras.preprocessing.sequence.pad_sequences(pred_text, maxlen=16, padding='post', truncating='post') #each text to predict has to be trimmed aswell

print(pred_text)

#just testing how to do function they want same code below

# function to predict messages based on model
# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])
def predict_message(pred_text):
  pred_text  = tokenizer.texts_to_sequences([pred_text])

  #pred_text = [[elem for sublist in pred_text for elem in sublist]]
  print(pred_text)
  pred_text = keras.preprocessing.sequence.pad_sequences(pred_text, maxlen=16, padding='post', truncating='post')

  #print(pred_text)

  prediction = model.predict(pred_text)
  #print(prediction) its in a list in a list (2d list) thats why double index to take the value below
  if prediction[0][0] > 0.3: #this threshold seems to work the best (0.3)
    label = 'spam'
  else:
    label = 'ham'
  print(prediction[0][0], label) 
  return (prediction[0][0], label)

pred_text = "hello your computer has virus pay 10 dollars to get this for free" 
prediction = predict_message(pred_text)
print(prediction)

# Run this cell to test your function and model. Do not modify contents.
def test_predictions():
  test_messages = ["how are you doing today",
                   "sale today! to stop texts call 98912460324",
                   "i dont want to go. can we try it a different day? available sat",
                   "our new mobile video service is live. just install on your phone to start watching.",
                   "you have won Â£1000 cash! call to claim your prize.",
                   "i'll bring it tomorrow. don't forget the milk.",
                   "wow, is your arm alright. that happened to me one time too"
                  ]

  test_answers = ["ham", "spam", "ham", "spam", "spam", "ham", "ham"]
  passed = True

  for msg, ans in zip(test_messages, test_answers):
    prediction = predict_message(msg)
    if prediction[1] != ans:
      passed = False

  if passed:
    print("You passed the challenge. Great job!")
  else:
    print("You haven't passed yet. Keep trying.")

test_predictions()