{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece1aaf6",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-12-31T13:01:34.414166Z",
     "iopub.status.busy": "2022-12-31T13:01:34.413607Z",
     "iopub.status.idle": "2022-12-31T13:02:48.205847Z",
     "shell.execute_reply": "2022-12-31T13:02:48.204091Z"
    },
    "id": "8RZOuS9LWQvv",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "2ce17c40-fa05-4eb3-e58f-a06cc79ca6c7",
    "papermill": {
     "duration": 73.803215,
     "end_time": "2022-12-31T13:02:48.208654",
     "exception": false,
     "start_time": "2022-12-31T13:01:34.405439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-nightly\r\n",
      "  Downloading tf_nightly-2.12.0.dev20221213-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (556.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.6/556.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (3.7.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (1.43.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (1.21.6)\r\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\r\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.29.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (21.3)\r\n",
      "Collecting keras-nightly~=2.12.0.dev\r\n",
      "  Downloading keras_nightly-2.12.0.dev2022121908-py2.py3-none-any.whl (1.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (3.3.0)\r\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (3.19.4)\r\n",
      "Collecting libclang>=13.0.0\r\n",
      "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (0.4.0)\r\n",
      "Collecting tf-estimator-nightly~=2.12.0.dev\r\n",
      "  Downloading tf_estimator_nightly-2.12.0.dev2022123109-py2.py3-none-any.whl (439 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.4/439.4 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (1.15.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (4.4.0)\r\n",
      "Collecting tb-nightly~=2.12.0.a\r\n",
      "  Downloading tb_nightly-2.12.0a20221230-py3-none-any.whl (5.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (0.2.0)\r\n",
      "Collecting absl-py>=1.0.0\r\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (59.8.0)\r\n",
      "Collecting flatbuffers>=2.0\r\n",
      "  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (1.12.1)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (1.1.0)\r\n",
      "Requirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (0.3.23)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (1.6.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tf-nightly) (0.37.1)\r\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/conda/lib/python3.7/site-packages (from jax>=0.3.15->tf-nightly) (1.7.3)\r\n",
      "Requirement already satisfied: etils[epath] in /opt/conda/lib/python3.7/site-packages (from jax>=0.3.15->tf-nightly) (0.8.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.12.0.a->tf-nightly) (1.35.0)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.12.0.a->tf-nightly) (2.2.2)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.12.0.a->tf-nightly) (2.28.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.12.0.a->tf-nightly) (1.8.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.12.0.a->tf-nightly) (0.4.6)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.12.0.a->tf-nightly) (0.6.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.12.0.a->tf-nightly) (3.3.7)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tf-nightly) (3.0.9)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.12.0.a->tf-nightly) (4.2.4)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.12.0.a->tf-nightly) (4.8)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.12.0.a->tf-nightly) (0.2.7)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.12.0.a->tf-nightly) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tb-nightly~=2.12.0.a->tf-nightly) (4.13.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.12.0.a->tf-nightly) (2022.9.24)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.12.0.a->tf-nightly) (2.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.12.0.a->tf-nightly) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.12.0.a->tf-nightly) (1.26.12)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tb-nightly~=2.12.0.a->tf-nightly) (2.1.1)\r\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.7/site-packages (from etils[epath]->jax>=0.3.15->tf-nightly) (5.8.0)\r\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.7/site-packages (from etils[epath]->jax>=0.3.15->tf-nightly) (3.8.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.12.0.a->tf-nightly) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.12.0.a->tf-nightly) (3.2.0)\r\n",
      "Installing collected packages: libclang, flatbuffers, tf-estimator-nightly, tensorflow-io-gcs-filesystem, keras-nightly, absl-py, tb-nightly, tf-nightly\r\n",
      "  Attempting uninstall: flatbuffers\r\n",
      "    Found existing installation: flatbuffers 1.12\r\n",
      "    Uninstalling flatbuffers-1.12:\r\n",
      "      Successfully uninstalled flatbuffers-1.12\r\n",
      "  Attempting uninstall: absl-py\r\n",
      "    Found existing installation: absl-py 0.15.0\r\n",
      "    Uninstalling absl-py-0.15.0:\r\n",
      "      Successfully uninstalled absl-py-0.15.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tfx-bsl 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\r\n",
      "tfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "tensorflow 2.6.4 requires absl-py~=0.10, but you have absl-py 1.3.0 which is incompatible.\r\n",
      "tensorflow 2.6.4 requires flatbuffers~=1.12.0, but you have flatbuffers 22.12.6 which is incompatible.\r\n",
      "tensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\r\n",
      "tensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\r\n",
      "tensorflow 2.6.4 requires tensorboard<2.7,>=2.6.0, but you have tensorboard 2.10.1 which is incompatible.\r\n",
      "tensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.4.0 which is incompatible.\r\n",
      "tensorflow-transform 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\r\n",
      "tensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "tensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.29.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed absl-py-1.3.0 flatbuffers-22.12.6 keras-nightly-2.12.0.dev2022121908 libclang-14.0.6 tb-nightly-2.12.0a20221230 tensorflow-io-gcs-filesystem-0.29.0 tf-estimator-nightly-2.12.0.dev2022123109 tf-nightly-2.12.0.dev20221213\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-31 13:02:30.316430: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-31 13:02:30.590842: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n",
      "2022-12-31 13:02:30.590907: I tensorflow/tsl/cuda/cudart_stub.cc:28] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-31 13:02:30.636737: E tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay\n",
      "2022-12-31 13:02:32.144440: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer.so.8'; dlerror: libnvinfer.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n",
      "2022-12-31 13:02:32.144595: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer_plugin.so.8'; dlerror: libnvinfer_plugin.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n",
      "2022-12-31 13:02:32.144607: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.7/site-packages (4.3.0)\r\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (3.19.4)\r\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (5.8.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (0.18.2)\r\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (1.9.0)\r\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (1.1.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (4.64.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (1.21.6)\r\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (2.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (2.28.1)\r\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (21.4.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (1.15.0)\r\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (1.3.0)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (0.3.5.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (4.4.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (2022.9.24)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.12)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources->tensorflow-datasets) (3.8.0)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.56.3)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m2.12.0-dev20221213\n"
     ]
    }
   ],
   "source": [
    "  # import libraries\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    !pip install tf-nightly\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "!pip install tensorflow-datasets\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4f638f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:48.249468Z",
     "iopub.status.busy": "2022-12-31T13:02:48.248420Z",
     "iopub.status.idle": "2022-12-31T13:02:49.374587Z",
     "shell.execute_reply": "2022-12-31T13:02:49.373289Z"
    },
    "id": "lMHwYXHXCar3",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d2ece07e-8d90-41dc-bd3f-dbce5fc7ae50",
    "papermill": {
     "duration": 1.149696,
     "end_time": "2022-12-31T13:02:49.377331",
     "exception": false,
     "start_time": "2022-12-31T13:02:48.227635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-31 13:02:48--  https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\r\n",
      "Resolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 104.26.3.33, 104.26.2.33, 172.67.70.149, ...\r\n",
      "Connecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|104.26.3.33|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 358233 (350K) [text/tab-separated-values]\r\n",
      "Saving to: ‘train-data.tsv’\r\n",
      "\r\n",
      "train-data.tsv      100%[===================>] 349.84K  --.-KB/s    in 0.04s   \r\n",
      "\r\n",
      "2022-12-31 13:02:48 (7.64 MB/s) - ‘train-data.tsv’ saved [358233/358233]\r\n",
      "\r\n",
      "--2022-12-31 13:02:49--  https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\r\n",
      "Resolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 104.26.3.33, 172.67.70.149, 104.26.2.33, ...\r\n",
      "Connecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|104.26.3.33|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 118774 (116K) [text/tab-separated-values]\r\n",
      "Saving to: ‘valid-data.tsv’\r\n",
      "\r\n",
      "valid-data.tsv      100%[===================>] 115.99K  --.-KB/s    in 0.02s   \r\n",
      "\r\n",
      "2022-12-31 13:02:49 (4.57 MB/s) - ‘valid-data.tsv’ saved [118774/118774]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# get data files\n",
    "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
    "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
    "\n",
    "train_file_path = \"train-data.tsv\"\n",
    "test_file_path = \"valid-data.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533f11f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:49.417722Z",
     "iopub.status.busy": "2022-12-31T13:02:49.417279Z",
     "iopub.status.idle": "2022-12-31T13:02:49.446987Z",
     "shell.execute_reply": "2022-12-31T13:02:49.445802Z"
    },
    "id": "g_h508FEClxO",
    "papermill": {
     "duration": 0.052418,
     "end_time": "2022-12-31T13:02:49.449504",
     "exception": false,
     "start_time": "2022-12-31T13:02:49.397086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "traindata = pd.read_table(train_file_path)\n",
    "vailddata = pd.read_table(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c821813a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:49.487466Z",
     "iopub.status.busy": "2022-12-31T13:02:49.487048Z",
     "iopub.status.idle": "2022-12-31T13:02:49.511034Z",
     "shell.execute_reply": "2022-12-31T13:02:49.509915Z"
    },
    "id": "NMR82r6f3GzR",
    "outputId": "a7a8a1ae-c7a1-48ff-c774-4aed08488464",
    "papermill": {
     "duration": 0.045968,
     "end_time": "2022-12-31T13:02:49.513914",
     "exception": false,
     "start_time": "2022-12-31T13:02:49.467946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>i am in hospital da. . i will return home in evening</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>not much, just some textin'. how bout you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>i probably won't eat at all today. i think i'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>don‘t give a flying monkeys wot they think and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>who are you seeing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>your opinion about me? 1. over 2. jada 3. kusr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ham i am in hospital da. . i will return home in evening\n",
       "0  ham         not much, just some textin'. how bout you?  \n",
       "1  ham  i probably won't eat at all today. i think i'm...  \n",
       "2  ham  don‘t give a flying monkeys wot they think and...  \n",
       "3  ham                                who are you seeing?  \n",
       "4  ham  your opinion about me? 1. over 2. jada 3. kusr...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vailddata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cde6db6",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:49.554500Z",
     "iopub.status.busy": "2022-12-31T13:02:49.554113Z",
     "iopub.status.idle": "2022-12-31T13:02:49.568638Z",
     "shell.execute_reply": "2022-12-31T13:02:49.566945Z"
    },
    "id": "tI9yMRix3NHx",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "866c728a-4b26-4353-daf7-68d4a77d2b15",
    "papermill": {
     "duration": 0.038552,
     "end_time": "2022-12-31T13:02:49.571566",
     "exception": false,
     "start_time": "2022-12-31T13:02:49.533014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>you can never do nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>now u sound like manky scouse boy steve,like! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>mum say we wan to go then go... then she can s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>never y lei... i v lazy... got wat? dat day ü ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>in xam hall boy asked girl tell me the startin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classification                                            message\n",
       "0            ham                           you can never do nothing\n",
       "1            ham  now u sound like manky scouse boy steve,like! ...\n",
       "2            ham  mum say we wan to go then go... then she can s...\n",
       "3            ham  never y lei... i v lazy... got wat? dat day ü ...\n",
       "4            ham  in xam hall boy asked girl tell me the startin..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i think by doing columns this way i lose the first entry but well its not a big deal i think\n",
    "traindata.columns = ['Classification', 'message']\n",
    "vailddata.columns = ['Classification', 'message']\n",
    "\n",
    "traindata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee14fb68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:49.612595Z",
     "iopub.status.busy": "2022-12-31T13:02:49.611315Z",
     "iopub.status.idle": "2022-12-31T13:02:49.624154Z",
     "shell.execute_reply": "2022-12-31T13:02:49.622530Z"
    },
    "id": "80HSo1OzXbAC",
    "papermill": {
     "duration": 0.036224,
     "end_time": "2022-12-31T13:02:49.626813",
     "exception": false,
     "start_time": "2022-12-31T13:02:49.590589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fulldata = traindata.message.append(vailddata.message, ignore_index=True) #i combine both datasets because i want to fill tokenizer (which i explain later) on entire data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ef23ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:49.668426Z",
     "iopub.status.busy": "2022-12-31T13:02:49.668053Z",
     "iopub.status.idle": "2022-12-31T13:02:49.692011Z",
     "shell.execute_reply": "2022-12-31T13:02:49.690692Z"
    },
    "id": "rwBV0DvZKPAF",
    "papermill": {
     "duration": 0.048983,
     "end_time": "2022-12-31T13:02:49.694877",
     "exception": false,
     "start_time": "2022-12-31T13:02:49.645894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = fulldata.str.split().apply(len) #Later i will need to know how many words emailes have, because i need them to have the same number of words i think good idea is to take mean number of words and trim excess words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ea9bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:49.737163Z",
     "iopub.status.busy": "2022-12-31T13:02:49.736737Z",
     "iopub.status.idle": "2022-12-31T13:02:49.749566Z",
     "shell.execute_reply": "2022-12-31T13:02:49.748035Z"
    },
    "id": "TbZC4M00KaWV",
    "outputId": "978925da-2052-49da-bcda-ec949f77bea8",
    "papermill": {
     "duration": 0.038238,
     "end_time": "2022-12-31T13:02:49.752800",
     "exception": false,
     "start_time": "2022-12-31T13:02:49.714562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n",
      "23.0\n"
     ]
    }
   ],
   "source": [
    "print(max(a))\n",
    "print(a.quantile(0.75)) #actually i decided to go with 3th quantile, where 75% of data is below this number and only 25% is excess\n",
    "\n",
    "length_of_emails = int(a.quantile(0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9541af3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:49.796020Z",
     "iopub.status.busy": "2022-12-31T13:02:49.795593Z",
     "iopub.status.idle": "2022-12-31T13:02:49.800544Z",
     "shell.execute_reply": "2022-12-31T13:02:49.799588Z"
    },
    "id": "J9RFc437Xi3T",
    "outputId": "aad7b762-14c1-448a-93f9-c3eb6d0cb4a9",
    "papermill": {
     "duration": 0.029214,
     "end_time": "2022-12-31T13:02:49.802781",
     "exception": false,
     "start_time": "2022-12-31T13:02:49.773567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5569\n"
     ]
    }
   ],
   "source": [
    "print(len(fulldata)) #not important but we have 5569 messages in total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1962607c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:49.844458Z",
     "iopub.status.busy": "2022-12-31T13:02:49.844084Z",
     "iopub.status.idle": "2022-12-31T13:02:50.112181Z",
     "shell.execute_reply": "2022-12-31T13:02:50.111048Z"
    },
    "id": "LK81Ixm6M_pN",
    "papermill": {
     "duration": 0.291756,
     "end_time": "2022-12-31T13:02:50.115046",
     "exception": false,
     "start_time": "2022-12-31T13:02:49.823290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Initialize tokenizer, what this does is essentialy it maps each word to some number, so we get a full vocabulary of numbers where each is some word. its important cuz you can,t do anything with text strings\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the Tokenizer on the entire data so it has all the words in vocab\n",
    "tokenizer.fit_on_texts(fulldata)\n",
    "\n",
    "# Convert the training data to sequences of tokens\n",
    "train_X = tokenizer.texts_to_sequences(traindata.message)\n",
    "\n",
    "# Convert the validation data to sequences of tokens\n",
    "val_X = tokenizer.texts_to_sequences(vailddata.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86fd7000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:50.158260Z",
     "iopub.status.busy": "2022-12-31T13:02:50.157869Z",
     "iopub.status.idle": "2022-12-31T13:02:50.163530Z",
     "shell.execute_reply": "2022-12-31T13:02:50.162538Z"
    },
    "id": "AikHQrBHNEHN",
    "outputId": "d92a25d8-508b-41b4-d2fb-6d008c7ff351",
    "papermill": {
     "duration": 0.028008,
     "end_time": "2022-12-31T13:02:50.165713",
     "exception": false,
     "start_time": "2022-12-31T13:02:50.137705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 122, 37, 116, 7819, 50, 742, 3]\n",
      "not much, just some textin'. how bout you?\n"
     ]
    }
   ],
   "source": [
    "print(val_X[0]) #it looks like this\n",
    "print(vailddata.message.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4c4faf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:50.209707Z",
     "iopub.status.busy": "2022-12-31T13:02:50.208754Z",
     "iopub.status.idle": "2022-12-31T13:02:50.236324Z",
     "shell.execute_reply": "2022-12-31T13:02:50.233528Z"
    },
    "id": "x6JQoWlrLANN",
    "papermill": {
     "duration": 0.054215,
     "end_time": "2022-12-31T13:02:50.239946",
     "exception": false,
     "start_time": "2022-12-31T13:02:50.185731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now i use this method to trim any email that has more than a mean amount of words in data (16) , padding fills this array of emails as numbers with 0, where 'post' means fill at the end if its less than 16. \n",
    "#truncating trims emails with more than 16 words, again post meaning trim at the end.  'pre' would trim at the beginning.\n",
    "train_X = keras.preprocessing.sequence.pad_sequences(train_X, maxlen=length_of_emails, padding='post', truncating='post') \n",
    "val_X =  keras.preprocessing.sequence.pad_sequences(val_X, maxlen=length_of_emails, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "446b3588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:50.281171Z",
     "iopub.status.busy": "2022-12-31T13:02:50.280689Z",
     "iopub.status.idle": "2022-12-31T13:02:50.289700Z",
     "shell.execute_reply": "2022-12-31T13:02:50.288175Z"
    },
    "id": "O-ISQ2kVNVq8",
    "outputId": "b636b424-f417-44d1-bb71-241eca184abd",
    "papermill": {
     "duration": 0.032058,
     "end_time": "2022-12-31T13:02:50.291878",
     "exception": false,
     "start_time": "2022-12-31T13:02:50.259820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  25  122   37 ...    0    0    0]\n",
      " [   1  388  569 ...    0    0    0]\n",
      " [1683  138    4 ...    0    0    0]\n",
      " ...\n",
      " [ 131   22    3 ...    4 8993   20]\n",
      " [  33  158  856 ... 1686 8994 1020]\n",
      " [  25  958   47 ... 1083 4419   57]]\n"
     ]
    }
   ],
   "source": [
    "print(val_X) #weird but ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03559192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:50.332366Z",
     "iopub.status.busy": "2022-12-31T13:02:50.331960Z",
     "iopub.status.idle": "2022-12-31T13:02:50.339239Z",
     "shell.execute_reply": "2022-12-31T13:02:50.337361Z"
    },
    "id": "J5BxPaK2LA9d",
    "outputId": "7a05f236-4bf5-4742-e12f-562a22ca92d2",
    "papermill": {
     "duration": 0.030862,
     "end_time": "2022-12-31T13:02:50.342112",
     "exception": false,
     "start_time": "2022-12-31T13:02:50.311250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8995\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1 #soo here is how to get vocabulary size out of this tokenizer. as you see it has more entries than fulldata words but this is because it has all the unique words, earlier we counted\n",
    "#just a total number of words\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68068135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:50.383472Z",
     "iopub.status.busy": "2022-12-31T13:02:50.383053Z",
     "iopub.status.idle": "2022-12-31T13:02:50.389742Z",
     "shell.execute_reply": "2022-12-31T13:02:50.387685Z"
    },
    "id": "g9aG_ynZ5cfo",
    "papermill": {
     "duration": 0.030759,
     "end_time": "2022-12-31T13:02:50.392612",
     "exception": false,
     "start_time": "2022-12-31T13:02:50.361853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_Y = traindata.Classification #remove labels from training and validation data\n",
    "val_Y = vailddata.Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c881e78c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:50.434159Z",
     "iopub.status.busy": "2022-12-31T13:02:50.433722Z",
     "iopub.status.idle": "2022-12-31T13:02:50.449127Z",
     "shell.execute_reply": "2022-12-31T13:02:50.447958Z"
    },
    "id": "9WOFteL359uB",
    "outputId": "0dda7811-8261-4380-c8ee-45b70ab1b139",
    "papermill": {
     "duration": 0.038935,
     "end_time": "2022-12-31T13:02:50.451292",
     "exception": false,
     "start_time": "2022-12-31T13:02:50.412357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: Classification, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Y.replace(to_replace = ['ham','spam'], value = [0,1],inplace = True) #now labels are string, again they need to be numbers so replace ham with 0 and spam with 1\n",
    "train_Y.replace(to_replace = ['ham','spam'], value = [0,1], inplace = True)\n",
    "train_Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d102b489",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:50.491407Z",
     "iopub.status.busy": "2022-12-31T13:02:50.490948Z",
     "iopub.status.idle": "2022-12-31T13:02:50.722241Z",
     "shell.execute_reply": "2022-12-31T13:02:50.720547Z"
    },
    "id": "faYocO9dVq-q",
    "papermill": {
     "duration": 0.254382,
     "end_time": "2022-12-31T13:02:50.724640",
     "exception": false,
     "start_time": "2022-12-31T13:02:50.470258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #its good to have a test data so i take a small fraction of train data to get test data by using this method, pretty self explainatory.\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(train_X, train_Y, random_state=2, test_size = 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e36b4542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:50.765311Z",
     "iopub.status.busy": "2022-12-31T13:02:50.764936Z",
     "iopub.status.idle": "2022-12-31T13:02:50.772089Z",
     "shell.execute_reply": "2022-12-31T13:02:50.769895Z"
    },
    "id": "XuLhy82YWPFx",
    "outputId": "681da715-fd88-4ccc-e676-84f950622edf",
    "papermill": {
     "duration": 0.030457,
     "end_time": "2022-12-31T13:02:50.774860",
     "exception": false,
     "start_time": "2022-12-31T13:02:50.744403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3760\n",
      "418\n",
      "1391\n"
     ]
    }
   ],
   "source": [
    "print(len(train_X))\n",
    "print(len(test_X))\n",
    "print(len(val_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1143e00c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:50.816901Z",
     "iopub.status.busy": "2022-12-31T13:02:50.816440Z",
     "iopub.status.idle": "2022-12-31T13:02:51.142137Z",
     "shell.execute_reply": "2022-12-31T13:02:51.140254Z"
    },
    "id": "4h6NoAyGOgsd",
    "outputId": "3ccc96b8-655d-4ef2-e05b-8a738cfe6fb9",
    "papermill": {
     "duration": 0.350228,
     "end_time": "2022-12-31T13:02:51.145303",
     "exception": false,
     "start_time": "2022-12-31T13:02:50.795075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-31 13:02:50.837729: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n",
      "2022-12-31 13:02:50.837782: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-31 13:02:50.837808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (6e6a0bc69f35): /proc/driver/nvidia/version does not exist\n",
      "2022-12-31 13:02:50.838263: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 23, 75)            674625    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               339968    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,014,850\n",
      "Trainable params: 1,014,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#okay the model, \n",
    "model = keras.Sequential([\n",
    "    \n",
    "  keras.layers.Embedding(input_dim=vocab_size, output_dim=75, input_length=length_of_emails), #generally it preprocess the data so the integers are mapped into vectors , output dim lets you choose how many numbers this vector has\n",
    "  keras.layers.LSTM(units=256, dropout=0.3, recurrent_dropout=0.3), #\n",
    "  keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001),metrics=[ 'acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5522520e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:02:51.190939Z",
     "iopub.status.busy": "2022-12-31T13:02:51.190548Z",
     "iopub.status.idle": "2022-12-31T13:06:16.229154Z",
     "shell.execute_reply": "2022-12-31T13:06:16.226935Z"
    },
    "id": "2z5zoYECPoPO",
    "outputId": "c0d76167-7ca3-4b1a-8794-45283c372e8d",
    "papermill": {
     "duration": 205.161377,
     "end_time": "2022-12-31T13:06:16.329183",
     "exception": false,
     "start_time": "2022-12-31T13:02:51.167806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "118/118 [==============================] - 18s 127ms/step - loss: 0.1566 - acc: 0.9463 - val_loss: 0.0724 - val_acc: 0.9784\n",
      "Epoch 2/10\n",
      "118/118 [==============================] - 20s 170ms/step - loss: 0.0393 - acc: 0.9894 - val_loss: 0.0773 - val_acc: 0.9777\n",
      "Epoch 3/10\n",
      "118/118 [==============================] - 16s 137ms/step - loss: 0.0171 - acc: 0.9960 - val_loss: 0.0569 - val_acc: 0.9871\n",
      "Epoch 4/10\n",
      "118/118 [==============================] - 15s 131ms/step - loss: 0.0086 - acc: 0.9979 - val_loss: 0.0547 - val_acc: 0.9835\n",
      "Epoch 5/10\n",
      "118/118 [==============================] - 14s 117ms/step - loss: 0.0126 - acc: 0.9976 - val_loss: 0.0586 - val_acc: 0.9842\n",
      "Epoch 6/10\n",
      "118/118 [==============================] - 13s 112ms/step - loss: 0.0028 - acc: 0.9995 - val_loss: 0.0499 - val_acc: 0.9863\n",
      "Epoch 7/10\n",
      "118/118 [==============================] - 14s 119ms/step - loss: 4.3608e-04 - acc: 1.0000 - val_loss: 0.1019 - val_acc: 0.9849\n",
      "Epoch 8/10\n",
      "118/118 [==============================] - 14s 115ms/step - loss: 1.2566e-04 - acc: 1.0000 - val_loss: 0.1392 - val_acc: 0.9799\n",
      "Epoch 9/10\n",
      "118/118 [==============================] - 14s 117ms/step - loss: 0.0029 - acc: 0.9997 - val_loss: 0.0755 - val_acc: 0.9856\n",
      "Epoch 10/10\n",
      "118/118 [==============================] - 13s 113ms/step - loss: 0.0098 - acc: 0.9984 - val_loss: 0.1066 - val_acc: 0.9813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d580be8d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, epochs=10, batch_size = 32,validation_data=(val_X, val_Y)) #train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db19e370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:06:16.517155Z",
     "iopub.status.busy": "2022-12-31T13:06:16.516483Z",
     "iopub.status.idle": "2022-12-31T13:06:17.215473Z",
     "shell.execute_reply": "2022-12-31T13:06:17.214365Z"
    },
    "id": "CtMAva5zqtHL",
    "outputId": "e8e27d72-8ea3-4779-9266-ef44284647c7",
    "papermill": {
     "duration": 0.798046,
     "end_time": "2022-12-31T13:06:17.217881",
     "exception": false,
     "start_time": "2022-12-31T13:06:16.419835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_X) #predictions on test data to see how good model is later\n",
    "y_prediction = []\n",
    "for i in y_pred:\n",
    "  if i > 0.3:\n",
    "    y_prediction.append(1)\n",
    "  else:\n",
    "    y_prediction.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80bc4fb9",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-12-31T13:06:17.402558Z",
     "iopub.status.busy": "2022-12-31T13:06:17.401233Z",
     "iopub.status.idle": "2022-12-31T13:06:17.414692Z",
     "shell.execute_reply": "2022-12-31T13:06:17.412147Z"
    },
    "id": "uvMdLQtmrvmp",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "be57049e-adbc-4335-97af-669083996a15",
    "papermill": {
     "duration": 0.110128,
     "end_time": "2022-12-31T13:06:17.418646",
     "exception": false,
     "start_time": "2022-12-31T13:06:17.308518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "1362    0\n",
      "586     0\n",
      "998     0\n",
      "1561    0\n",
      "171     1\n",
      "       ..\n",
      "371     0\n",
      "3377    0\n",
      "1695    0\n",
      "3964    0\n",
      "1267    0\n",
      "Name: Classification, Length: 418, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_prediction)\n",
    "print(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5212a918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:06:17.605282Z",
     "iopub.status.busy": "2022-12-31T13:06:17.604908Z",
     "iopub.status.idle": "2022-12-31T13:06:17.617529Z",
     "shell.execute_reply": "2022-12-31T13:06:17.616172Z"
    },
    "id": "4iJqRw8QqqtR",
    "outputId": "868b4e28-9476-4ced-acaa-97436f4a5a83",
    "papermill": {
     "duration": 0.106786,
     "end_time": "2022-12-31T13:06:17.620057",
     "exception": false,
     "start_time": "2022-12-31T13:06:17.513271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.887\n",
      "Recall: 0.948\n",
      "F1 score the (higher the better between 0 and 1) is:  0.917\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# so this function allows me to see precision and recall\n",
    "#precision is basically number of  true positive (predictions belonging to class closer to 1 in binary classification , which are correct)/all the positives model made eg. 80/100 means 80% precision and that 80 of 100 predictions for eg of spam were correct.\n",
    "#recall is  number of true positves but this time divided by an actual number of positives so lets say we have 100 spam emails. if model predicts 70 of them then recall is 70/100 = 70%\n",
    "#f1 score is just some math including previous metrics considered as good indicator \n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(test_Y, y_prediction, average='binary')\n",
    "\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f'F1 score the (higher the better between 0 and 1) is:  {f1_score:.3f}')\n",
    "\n",
    "#seems good\n",
    "\n",
    "#IMPORTANT, due to this task making me predict specific emails, i see that when i set the threshold of whether an email is a spam or not to 0.3 i cannot complete the task but it typically has a better F1 score.\n",
    "#when i set it to 0.0001 the task is completed but the F1 score drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc0e9308",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:06:17.809534Z",
     "iopub.status.busy": "2022-12-31T13:06:17.808793Z",
     "iopub.status.idle": "2022-12-31T13:06:17.816300Z",
     "shell.execute_reply": "2022-12-31T13:06:17.814748Z"
    },
    "id": "ecsI57iTGXqG",
    "outputId": "6a722fb8-3b86-4b3f-a101-7172b38388cf",
    "papermill": {
     "duration": 0.106809,
     "end_time": "2022-12-31T13:06:17.819332",
     "exception": false,
     "start_time": "2022-12-31T13:06:17.712523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50, 22, 3, 159, 89]]\n",
      "[[ 50  22   3 159  89   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_text = [\"how are you doing today?\"] #it needs to be in a list or tokenizer encodes each letter\n",
    "pred_text  = tokenizer.texts_to_sequences(pred_text)\n",
    "#pred_text = [[elem for sublist in pred_text for elem in sublist]] it flattened some version of a list no longer neccasary\n",
    "print(pred_text)\n",
    "pred_text = keras.preprocessing.sequence.pad_sequences(pred_text, maxlen=16, padding='post', truncating='post') #each text to predict has to be trimmed aswell\n",
    "\n",
    "print(pred_text)\n",
    "\n",
    "#just testing how to do function they want same code below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e8b6a81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:06:18.007962Z",
     "iopub.status.busy": "2022-12-31T13:06:18.007450Z",
     "iopub.status.idle": "2022-12-31T13:06:18.354307Z",
     "shell.execute_reply": "2022-12-31T13:06:18.352908Z"
    },
    "id": "J9tD9yACG6M9",
    "outputId": "be5a4f4d-2c2a-4003-be49-47cfddf246ed",
    "papermill": {
     "duration": 0.443085,
     "end_time": "2022-12-31T13:06:18.356983",
     "exception": false,
     "start_time": "2022-12-31T13:06:17.913898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[258, 13, 932, 118, 394, 431, 1692, 2, 31, 40, 12, 48]]\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "0.661164 spam\n",
      "(0.661164, 'spam')\n"
     ]
    }
   ],
   "source": [
    "# function to predict messages based on model\n",
    "# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n",
    "def predict_message(pred_text):\n",
    "  pred_text  = tokenizer.texts_to_sequences([pred_text])\n",
    "\n",
    "  #pred_text = [[elem for sublist in pred_text for elem in sublist]]\n",
    "  print(pred_text)\n",
    "  pred_text = keras.preprocessing.sequence.pad_sequences(pred_text, maxlen=16, padding='post', truncating='post')\n",
    "\n",
    "  #print(pred_text)\n",
    "\n",
    "  prediction = model.predict(pred_text)\n",
    "  #print(prediction) its in a list in a list (2d list) thats why double index to take the value below\n",
    "  if prediction[0][0] > 0.3: #this threshold seems to work the best (0.3)\n",
    "    label = 'spam'\n",
    "  else:\n",
    "    label = 'ham'\n",
    "  print(prediction[0][0], label) \n",
    "  return (prediction[0][0], label)\n",
    "\n",
    "pred_text = \"hello your computer has virus pay 10 dollars to get this for free\" \n",
    "prediction = predict_message(pred_text)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55d07524",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T13:06:18.600232Z",
     "iopub.status.busy": "2022-12-31T13:06:18.599572Z",
     "iopub.status.idle": "2022-12-31T13:06:19.186447Z",
     "shell.execute_reply": "2022-12-31T13:06:19.184442Z"
    },
    "id": "Dxotov85SjsC",
    "outputId": "c17258ea-bb25-46a5-ac9f-94205766b6e9",
    "papermill": {
     "duration": 0.741167,
     "end_time": "2022-12-31T13:06:19.189470",
     "exception": false,
     "start_time": "2022-12-31T13:06:18.448303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50, 22, 3, 159, 89]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "0.000698405 ham\n",
      "[[1248, 89, 2, 84, 469, 16]]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "0.9995712 spam\n",
      "[[1, 93, 70, 2, 49, 28, 39, 261, 14, 4, 1068, 64, 663, 408]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "0.006807847 ham\n",
      "[[92, 102, 97, 375, 230, 9, 287, 37, 8433, 18, 13, 111, 2, 345, 379]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "0.9999323 spam\n",
      "[[3, 17, 185, 370, 158, 16, 2, 128, 13, 152]]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "0.9999347 spam\n",
      "[[79, 498, 14, 151, 98, 665, 5, 2287]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "0.0009531549 ham\n",
      "[[926, 9, 13, 4283, 530, 20, 728, 2, 10, 74, 63, 134]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "0.00036374907 ham\n",
      "You passed the challenge. Great job!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function and model. Do not modify contents.\n",
    "def test_predictions():\n",
    "  test_messages = [\"how are you doing today\",\n",
    "                   \"sale today! to stop texts call 98912460324\",\n",
    "                   \"i dont want to go. can we try it a different day? available sat\",\n",
    "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
    "                   \"you have won £1000 cash! call to claim your prize.\",\n",
    "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
    "                   \"wow, is your arm alright. that happened to me one time too\"\n",
    "                  ]\n",
    "\n",
    "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
    "  passed = True\n",
    "\n",
    "  for msg, ans in zip(test_messages, test_answers):\n",
    "    prediction = predict_message(msg)\n",
    "    if prediction[1] != ans:\n",
    "      passed = False\n",
    "\n",
    "  if passed:\n",
    "    print(\"You passed the challenge. Great job!\")\n",
    "  else:\n",
    "    print(\"You haven't passed yet. Keep trying.\")\n",
    "\n",
    "test_predictions()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 297.268862,
   "end_time": "2022-12-31T13:06:22.682570",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-31T13:01:25.413708",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
