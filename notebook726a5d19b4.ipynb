{"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"  # import libraries\ntry:\n    # %tensorflow_version only exists in Colab.\n    !pip install tf-nightly\nexcept Exception:\n    pass\nimport tensorflow as tf\nimport pandas as pd\nfrom tensorflow import keras\n!pip install tensorflow-datasets\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RZOuS9LWQvv","outputId":"2ce17c40-fa05-4eb3-e58f-a06cc79ca6c7","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n\nRequirement already satisfied: tf-nightly in /usr/local/lib/python3.8/dist-packages (2.12.0.dev20221230)\n\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (4.4.0)\n\nRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (22.12.6)\n\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (57.4.0)\n\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (2.1.1)\n\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (14.0.6)\n\nRequirement already satisfied: numpy<1.24,>=1.20 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (1.21.6)\n\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (1.15.0)\n\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (3.19.6)\n\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (0.4.0)\n\nRequirement already satisfied: tf-estimator-nightly~=2.12.0.dev in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (2.12.0.dev2022123009)\n\nRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (21.3)\n\nRequirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (0.3.25)\n\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (0.2.0)\n\nRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (3.1.0)\n\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (1.14.1)\n\nRequirement already satisfied: tb-nightly~=2.12.0.a in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (2.12.0a20221230)\n\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (1.51.1)\n\nRequirement already satisfied: keras-nightly~=2.12.0.dev in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (2.12.0.dev2022123008)\n\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (0.28.0)\n\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (3.3.0)\n\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (1.3.0)\n\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tf-nightly) (1.6.3)\n\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tf-nightly) (0.38.4)\n\nRequirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->tf-nightly) (1.7.3)\n\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tb-nightly~=2.12.0.a->tf-nightly) (0.4.6)\n\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tb-nightly~=2.12.0.a->tf-nightly) (2.23.0)\n\nRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tb-nightly~=2.12.0.a->tf-nightly) (2.15.0)\n\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tb-nightly~=2.12.0.a->tf-nightly) (3.4.1)\n\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tb-nightly~=2.12.0.a->tf-nightly) (0.6.1)\n\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tb-nightly~=2.12.0.a->tf-nightly) (1.8.1)\n\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tb-nightly~=2.12.0.a->tf-nightly) (1.0.1)\n\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.12.0.a->tf-nightly) (5.2.0)\n\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.12.0.a->tf-nightly) (0.2.8)\n\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.12.0.a->tf-nightly) (4.9)\n\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.12.0.a->tf-nightly) (1.3.1)\n\nRequirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tb-nightly~=2.12.0.a->tf-nightly) (5.1.0)\n\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly~=2.12.0.a->tf-nightly) (3.11.0)\n\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.12.0.a->tf-nightly) (0.4.8)\n\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.12.0.a->tf-nightly) (2022.12.7)\n\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.12.0.a->tf-nightly) (3.0.4)\n\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.12.0.a->tf-nightly) (1.24.3)\n\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.12.0.a->tf-nightly) (2.10)\n\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.12.0.a->tf-nightly) (3.2.2)\n\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tf-nightly) (3.0.9)\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n\nRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (4.6.0)\n\nRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.15.0)\n\nRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (3.19.6)\n\nRequirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.3.6)\n\nRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (4.64.1)\n\nRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.12.0)\n\nRequirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.1.1)\n\nRequirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.10.2)\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.21.6)\n\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.23.0)\n\nRequirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.3)\n\nRequirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.3.0)\n\nRequirement already satisfied: etils[epath] in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.9.0)\n\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (5.10.1)\n\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2022.12.7)\n\nRequirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[epath]->tensorflow-datasets) (3.11.0)\n\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from etils[epath]->tensorflow-datasets) (4.4.0)\n\nRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.57.0)\n\n2.12.0-dev20221230\n"}]},{"cell_type":"code","source":"# get data files\n!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n\ntrain_file_path = \"train-data.tsv\"\ntest_file_path = \"valid-data.tsv\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lMHwYXHXCar3","outputId":"d2ece07e-8d90-41dc-bd3f-dbce5fc7ae50","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"--2022-12-30 20:50:17--  https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n\nResolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 172.67.70.149, 104.26.2.33, 104.26.3.33, ...\n\nConnecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|172.67.70.149|:443... connected.\n\nHTTP request sent, awaiting response... 200 OK\n\nLength: 358233 (350K) [text/tab-separated-values]\n\nSaving to: ‘train-data.tsv.6’\n\n\n\ntrain-data.tsv.6    100%[===================>] 349.84K  --.-KB/s    in 0.04s   \n\n\n\n2022-12-30 20:50:17 (8.88 MB/s) - ‘train-data.tsv.6’ saved [358233/358233]\n\n\n\n--2022-12-30 20:50:17--  https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n\nResolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 172.67.70.149, 104.26.2.33, 104.26.3.33, ...\n\nConnecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|172.67.70.149|:443... connected.\n\nHTTP request sent, awaiting response... 200 OK\n\nLength: 118774 (116K) [text/tab-separated-values]\n\nSaving to: ‘valid-data.tsv.6’\n\n\n\nvalid-data.tsv.6    100%[===================>] 115.99K  --.-KB/s    in 0.02s   \n\n\n\n2022-12-30 20:50:17 (4.80 MB/s) - ‘valid-data.tsv.6’ saved [118774/118774]\n\n\n"}]},{"cell_type":"code","source":"traindata = pd.read_table(train_file_path)\nvailddata = pd.read_table(test_file_path)","metadata":{"id":"g_h508FEClxO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vailddata.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"NMR82r6f3GzR","outputId":"a7a8a1ae-c7a1-48ff-c774-4aed08488464"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":105,"data":{"text/plain":["   ham i am in hospital da. . i will return home in evening\n","0  ham         not much, just some textin'. how bout you?  \n","1  ham  i probably won't eat at all today. i think i'm...  \n","2  ham  don‘t give a flying monkeys wot they think and...  \n","3  ham                                who are you seeing?  \n","4  ham  your opinion about me? 1. over 2. jada 3. kusr...  "],"text/html":["\n","  <div id=\"df-c1a3ffd6-2f17-4f79-93fd-5df041b4cd01\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ham</th>\n","      <th>i am in hospital da. . i will return home in evening</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>not much, just some textin'. how bout you?</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ham</td>\n","      <td>i probably won't eat at all today. i think i'm...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ham</td>\n","      <td>don‘t give a flying monkeys wot they think and...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>who are you seeing?</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>your opinion about me? 1. over 2. jada 3. kusr...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1a3ffd6-2f17-4f79-93fd-5df041b4cd01')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c1a3ffd6-2f17-4f79-93fd-5df041b4cd01 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c1a3ffd6-2f17-4f79-93fd-5df041b4cd01');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":"#i think by doing columns this way i lose the first entry but well its not a big deal i think\ntraindata.columns = ['Classification', 'message']\nvailddata.columns = ['Classification', 'message']\n\ntraindata.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"tI9yMRix3NHx","outputId":"866c728a-4b26-4353-daf7-68d4a77d2b15","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":106,"data":{"text/plain":["  Classification                                            message\n","0            ham                           you can never do nothing\n","1            ham  now u sound like manky scouse boy steve,like! ...\n","2            ham  mum say we wan to go then go... then she can s...\n","3            ham  never y lei... i v lazy... got wat? dat day ü ...\n","4            ham  in xam hall boy asked girl tell me the startin..."],"text/html":["\n","  <div id=\"df-a6ccb7f5-c1dd-41a8-8b38-0d49d85a6c3e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Classification</th>\n","      <th>message</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>you can never do nothing</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ham</td>\n","      <td>now u sound like manky scouse boy steve,like! ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ham</td>\n","      <td>mum say we wan to go then go... then she can s...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>never y lei... i v lazy... got wat? dat day ü ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>in xam hall boy asked girl tell me the startin...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6ccb7f5-c1dd-41a8-8b38-0d49d85a6c3e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a6ccb7f5-c1dd-41a8-8b38-0d49d85a6c3e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a6ccb7f5-c1dd-41a8-8b38-0d49d85a6c3e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":"fulldata = traindata.message.append(vailddata.message, ignore_index=True) #i combine both datasets because i want to fill tokenizer (which i explain later) on entire data\n","metadata":{"id":"80HSo1OzXbAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = fulldata.str.split().apply(len) #Later i will need to know how many words emailes have, because i need them to have the same number of words i think good idea is to take mean number of words and trim excess words","metadata":{"id":"rwBV0DvZKPAF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(max(a))\nprint(a.quantile(0.75)) #actually i decided to go with 3th quantile, where 75% of data is below this number and only 25% is excess\n\nlength_of_emails = int(a.quantile(0.75))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbZC4M00KaWV","outputId":"978925da-2052-49da-bcda-ec949f77bea8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"171\n\n23.0\n"}]},{"cell_type":"code","source":"print(len(fulldata)) #not important but we have 5569 messages in total ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J9RFc437Xi3T","outputId":"aad7b762-14c1-448a-93f9-c3eb6d0cb4a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"5569\n"}]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\n# Initialize tokenizer, what this does is essentialy it maps each word to some number, so we get a full vocabulary of numbers where each is some word. its important cuz you can,t do anything with text strings\ntokenizer = Tokenizer()\n\n# Fit the Tokenizer on the entire data so it has all the words in vocab\ntokenizer.fit_on_texts(fulldata)\n\n# Convert the training data to sequences of tokens\ntrain_X = tokenizer.texts_to_sequences(traindata.message)\n\n# Convert the validation data to sequences of tokens\nval_X = tokenizer.texts_to_sequences(vailddata.message)","metadata":{"id":"LK81Ixm6M_pN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(val_X[0]) #it looks like this\nprint(vailddata.message.loc[0])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AikHQrBHNEHN","outputId":"d92a25d8-508b-41b4-d2fb-6d008c7ff351"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"[25, 122, 37, 116, 7819, 50, 742, 3]\n\nnot much, just some textin'. how bout you?\n"}]},{"cell_type":"code","source":"#now i use this method to trim any email that has more than a mean amount of words in data (16) , padding fills this array of emails as numbers with 0, where 'post' means fill at the end if its less than 16. \n#truncating trims emails with more than 16 words, again post meaning trim at the end.  'pre' would trim at the beginning.\ntrain_X = keras.preprocessing.sequence.pad_sequences(train_X, maxlen=length_of_emails, padding='post', truncating='post') \nval_X =  keras.preprocessing.sequence.pad_sequences(val_X, maxlen=length_of_emails, padding='post', truncating='post')","metadata":{"id":"x6JQoWlrLANN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(val_X) #weird but ok","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-ISQ2kVNVq8","outputId":"b636b424-f417-44d1-bb71-241eca184abd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"[[  25  122   37 ...    0    0    0]\n\n [   1  388  569 ...    0    0    0]\n\n [1683  138    4 ...    0    0    0]\n\n ...\n\n [ 131   22    3 ...    4 8993   20]\n\n [  33  158  856 ... 1686 8994 1020]\n\n [  25  958   47 ... 1083 4419   57]]\n"}]},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1 #soo here is how to get vocabulary size out of this tokenizer. as you see it has more entries than fulldata words but this is because it has all the unique words, earlier we counted\n#just a total number of words\nprint(vocab_size)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J5BxPaK2LA9d","outputId":"7a05f236-4bf5-4742-e12f-562a22ca92d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"8995\n"}]},{"cell_type":"code","source":"\ntrain_Y = traindata.Classification #remove labels from training and validation data\nval_Y = vailddata.Classification","metadata":{"id":"g9aG_ynZ5cfo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_Y.replace(to_replace = ['ham','spam'], value = [0,1],inplace = True) #now labels are string, again they need to be numbers so replace ham with 0 and spam with 1\ntrain_Y.replace(to_replace = ['ham','spam'], value = [0,1], inplace = True)\ntrain_Y.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9WOFteL359uB","outputId":"0dda7811-8261-4380-c8ee-45b70ab1b139"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":117,"data":{"text/plain":["0    0\n","1    0\n","2    0\n","3    0\n","4    0\n","Name: Classification, dtype: int64"]},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split #its good to have a test data so i take a small fraction of train data to get test data by using this method, pretty self explainatory.\ntrain_X, test_X, train_Y, test_Y = train_test_split(train_X, train_Y, random_state=2, test_size = 0.1) ","metadata":{"id":"faYocO9dVq-q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_X))\nprint(len(test_X))\nprint(len(val_X))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuLhy82YWPFx","outputId":"681da715-fd88-4ccc-e676-84f950622edf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"3760\n\n418\n\n1391\n"}]},{"cell_type":"code","source":"#okay the model, \nmodel = keras.Sequential([\n    \n  keras.layers.Embedding(input_dim=vocab_size, output_dim=75, input_length=length_of_emails), #generally it preprocess the data so the integers are mapped into vectors , output dim lets you choose how many numbers this vector has\n  keras.layers.LSTM(units=256, dropout=0.3, recurrent_dropout=0.3), #\n  keras.layers.Dense(units=1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001),metrics=[ 'acc'])\nmodel.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4h6NoAyGOgsd","outputId":"3ccc96b8-655d-4ef2-e05b-8a738cfe6fb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Model: \"sequential_2\"\n\n_________________________________________________________________\n\n Layer (type)                Output Shape              Param #   \n\n=================================================================\n\n embedding_2 (Embedding)     (None, 23, 75)            674625    \n\n                                                                 \n\n lstm_2 (LSTM)               (None, 256)               339968    \n\n                                                                 \n\n dense_2 (Dense)             (None, 1)                 257       \n\n                                                                 \n\n=================================================================\n\nTotal params: 1,014,850\n\nTrainable params: 1,014,850\n\nNon-trainable params: 0\n\n_________________________________________________________________\n"}]},{"cell_type":"code","source":"model.fit(train_X, train_Y, epochs=10, batch_size = 32,validation_data=(val_X, val_Y)) #train the data","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2z5zoYECPoPO","outputId":"c0d76167-7ca3-4b1a-8794-45283c372e8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Epoch 1/10\n\n118/118 [==============================] - 39s 297ms/step - loss: 0.1533 - acc: 0.9492 - val_loss: 0.0681 - val_acc: 0.9799\n\nEpoch 2/10\n\n118/118 [==============================] - 27s 229ms/step - loss: 0.0491 - acc: 0.9883 - val_loss: 0.0615 - val_acc: 0.9842\n\nEpoch 3/10\n\n118/118 [==============================] - 30s 251ms/step - loss: 0.0088 - acc: 0.9981 - val_loss: 0.0636 - val_acc: 0.9885\n\nEpoch 4/10\n\n118/118 [==============================] - 29s 245ms/step - loss: 0.0142 - acc: 0.9955 - val_loss: 0.0564 - val_acc: 0.9878\n\nEpoch 5/10\n\n118/118 [==============================] - 26s 224ms/step - loss: 0.0070 - acc: 0.9981 - val_loss: 0.0764 - val_acc: 0.9871\n\nEpoch 6/10\n\n118/118 [==============================] - 26s 225ms/step - loss: 0.0029 - acc: 0.9997 - val_loss: 0.0677 - val_acc: 0.9878\n\nEpoch 7/10\n\n118/118 [==============================] - 27s 227ms/step - loss: 0.0026 - acc: 0.9997 - val_loss: 0.0770 - val_acc: 0.9871\n\nEpoch 8/10\n\n118/118 [==============================] - 28s 238ms/step - loss: 0.0109 - acc: 0.9989 - val_loss: 0.2156 - val_acc: 0.9669\n\nEpoch 9/10\n\n118/118 [==============================] - 26s 219ms/step - loss: 0.0184 - acc: 0.9965 - val_loss: 0.0936 - val_acc: 0.9849\n\nEpoch 10/10\n\n118/118 [==============================] - 26s 219ms/step - loss: 0.0090 - acc: 0.9984 - val_loss: 0.0895 - val_acc: 0.9863\n"},{"output_type":"execute_result","execution_count":121,"data":{"text/plain":["<keras.callbacks.History at 0x7f88a240feb0>"]},"metadata":{}}]},{"cell_type":"code","source":"y_pred = model.predict(test_X) #predictions on test data to see how good model is later\ny_prediction = []\nfor i in y_pred:\n  if i > 0.3:\n    y_prediction.append(1)\n  else:\n    y_prediction.append(0)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CtMAva5zqtHL","outputId":"e8e27d72-8ea3-4779-9266-ef44284647c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"14/14 [==============================] - 1s 53ms/step\n"}]},{"cell_type":"code","source":"print(y_prediction)\nprint(test_Y)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uvMdLQtmrvmp","outputId":"be57049e-adbc-4335-97af-669083996a15","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n\n1362    0\n\n586     0\n\n998     0\n\n1561    0\n\n171     1\n\n       ..\n\n371     0\n\n3377    0\n\n1695    0\n\n3964    0\n\n1267    0\n\nName: Classification, Length: 418, dtype: int64\n"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support\n\n# so this function allows me to see precision and recall\n#precision is basically number of  true positive (predictions belonging to class closer to 1 in binary classification , which are correct)/all the positives model made eg. 80/100 means 80% precision and that 80 of 100 predictions for eg of spam were correct.\n#recall is  number of true positves but this time divided by an actual number of positives so lets say we have 100 spam emails. if model predicts 70 of them then recall is 70/100 = 70%\n#f1 score is just some math including previous metrics considered as good indicator \nprecision, recall, f1_score, support = precision_recall_fscore_support(test_Y, y_prediction, average='binary')\n\nprint(f'Precision: {precision:.3f}')\nprint(f'Recall: {recall:.3f}')\n\nF1 = 2 * (precision * recall) / (precision + recall)\nprint(f'F1 score the (higher the better between 0 and 1) is:  {f1_score:.3f}')\n\n#seems good\n\n#IMPORTANT, due to this task making me predict specific emails, i see that when i set the threshold of whether an email is a spam or not to 0.3 i cannot complete the task but it typically has a better F1 score.\n#when i set it to 0.0001 the task is completed but the F1 score drops","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4iJqRw8QqqtR","outputId":"868b4e28-9476-4ced-acaa-97436f4a5a83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Precision: 0.917\n\nRecall: 0.948\n\nF1 score the (higher the better between 0 and 1) is:  0.932\n"}]},{"cell_type":"code","source":"\npred_text = [\"how are you doing today?\"] #it needs to be in a list or tokenizer encodes each letter\npred_text  = tokenizer.texts_to_sequences(pred_text)\n#pred_text = [[elem for sublist in pred_text for elem in sublist]] it flattened some version of a list no longer neccasary\nprint(pred_text)\npred_text = keras.preprocessing.sequence.pad_sequences(pred_text, maxlen=16, padding='post', truncating='post') #each text to predict has to be trimmed aswell\n\nprint(pred_text)\n\n#just testing how to do function they want same code below ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ecsI57iTGXqG","outputId":"6a722fb8-3b86-4b3f-a101-7172b38388cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"[[50, 22, 3, 159, 89]]\n\n[[ 50  22   3 159  89   0   0   0   0   0   0   0   0   0   0   0]]\n"}]},{"cell_type":"code","source":"# function to predict messages based on model\n# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\ndef predict_message(pred_text):\n  pred_text  = tokenizer.texts_to_sequences([pred_text])\n\n  #pred_text = [[elem for sublist in pred_text for elem in sublist]]\n  print(pred_text)\n  pred_text = keras.preprocessing.sequence.pad_sequences(pred_text, maxlen=16, padding='post', truncating='post')\n\n  #print(pred_text)\n\n  prediction = model.predict(pred_text)\n  #print(prediction) its in a list in a list (2d list) thats why double index to take the value below\n  if prediction[0][0] > 0.3: #this threshold seems to work the best (0.3)\n    label = 'spam'\n  else:\n    label = 'ham'\n  print(prediction[0][0], label) \n  return (prediction[0][0], label)\n\npred_text = \"hello your computer has virus pay 10 dollars to get this for free\" \nprediction = predict_message(pred_text)\nprint(prediction)","metadata":{"id":"J9tD9yACG6M9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"be5a4f4d-2c2a-4003-be49-47cfddf246ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"[[258, 13, 932, 118, 394, 431, 1692, 2, 31, 40, 12, 48]]\n\n1/1 [==============================] - 0s 152ms/step\n\n0.103817336 ham\n\n(0.103817336, 'ham')\n"}]},{"cell_type":"code","source":"# Run this cell to test your function and model. Do not modify contents.\ndef test_predictions():\n  test_messages = [\"how are you doing today\",\n                   \"sale today! to stop texts call 98912460324\",\n                   \"i dont want to go. can we try it a different day? available sat\",\n                   \"our new mobile video service is live. just install on your phone to start watching.\",\n                   \"you have won £1000 cash! call to claim your prize.\",\n                   \"i'll bring it tomorrow. don't forget the milk.\",\n                   \"wow, is your arm alright. that happened to me one time too\"\n                  ]\n\n  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n  passed = True\n\n  for msg, ans in zip(test_messages, test_answers):\n    prediction = predict_message(msg)\n    if prediction[1] != ans:\n      passed = False\n\n  if passed:\n    print(\"You passed the challenge. Great job!\")\n  else:\n    print(\"You haven't passed yet. Keep trying.\")\n\ntest_predictions()\n","metadata":{"id":"Dxotov85SjsC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c17258ea-bb25-46a5-ac9f-94205766b6e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"[[50, 22, 3, 159, 89]]\n\n1/1 [==============================] - 0s 27ms/step\n\n7.5539596e-05 ham\n\n[[1248, 89, 2, 84, 469, 16]]\n\n1/1 [==============================] - 0s 31ms/step\n\n0.99081236 spam\n\n[[1, 93, 70, 2, 49, 28, 39, 261, 14, 4, 1068, 64, 663, 408]]\n\n1/1 [==============================] - 0s 28ms/step\n\n0.0027869695 ham\n\n[[92, 102, 97, 375, 230, 9, 287, 37, 8433, 18, 13, 111, 2, 345, 379]]\n\n1/1 [==============================] - 0s 31ms/step\n\n0.9935156 spam\n\n[[3, 17, 185, 370, 158, 16, 2, 128, 13, 152]]\n\n1/1 [==============================] - 0s 33ms/step\n\n0.993478 spam\n\n[[79, 498, 14, 151, 98, 665, 5, 2287]]\n\n1/1 [==============================] - 0s 28ms/step\n\n2.7634576e-05 ham\n\n[[926, 9, 13, 4283, 530, 20, 728, 2, 10, 74, 63, 134]]\n\n1/1 [==============================] - 0s 26ms/step\n\n4.8516435e-05 ham\n\nYou passed the challenge. Great job!\n"}]}]}